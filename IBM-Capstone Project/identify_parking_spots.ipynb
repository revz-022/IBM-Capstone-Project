{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "def show_images(images, cmap=None):\n    cols = 2\n    rows = (len(images)+1)//cols\n    \n    plt.figure(figsize=(15, 12))\n    for i, image in enumerate(images):\n        plt.subplot(rows, cols, i+1)\n        # use gray scale color map if there is only one channel\n        cmap = 'gray' if len(image.shape)==2 else cmap\n        plt.imshow(image, cmap=cmap)\n        plt.xticks([])\n        plt.yticks([])\n    plt.tight_layout(pad=0, h_pad=0, w_pad=0)\n    plt.show()",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test_images = [plt.imread(path) for path in glob.glob('test_images/*.jpg')]\n\nshow_images(test_images)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Color Selection and Edge Detection",
      "metadata": {
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# image is expected be in RGB color space# image \ndef select_rgb_white_yellow(image): \n    # white color mask\n    lower = np.uint8([120, 120, 120])\n    upper = np.uint8([255, 255, 255])\n    white_mask = cv2.inRange(image, lower, upper)\n    # yellow color mask\n    lower = np.uint8([190, 190,   0])\n    upper = np.uint8([255, 255, 255])\n    yellow_mask = cv2.inRange(image, lower, upper)\n    # combine the mask\n    mask = cv2.bitwise_or(white_mask, yellow_mask)\n    masked = cv2.bitwise_and(image, image, mask = mask)\n    return masked\n\nwhite_yellow_images = list(map(select_rgb_white_yellow, test_images))\nshow_images(white_yellow_images)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Identify area of interest",
      "metadata": {
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Hough line transform",
      "metadata": {
        "trusted": true
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def draw_lines(image, lines, color=[255, 0, 0], thickness=2, make_copy=True):\n    # the lines returned by cv2.HoughLinesP has the shape (-1, 1, 4)\n    if make_copy:\n        image = np.copy(image) # don't want to modify the original\n    cleaned = []\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            if abs(y2-y1) <=1 and abs(x2-x1) >=25 and abs(x2-x1) <= 55:\n                cleaned.append((x1,y1,x2,y2))\n                cv2.line(image, (x1, y1), (x2, y2), color, thickness)\n    print(\" No lines detected: \", len(cleaned))\n    return image\n\nline_images = []\nfor image, lines in zip(test_images, list_of_lines):\n    line_images.append(draw_lines(image, lines))\n    \nshow_images(line_images)\n(' No lines detected: ', 569)\n(' No lines detected: ', 588)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Identify rectangular blocks of parking",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def identify_blocks(image, lines, make_copy=True):\n    if make_copy:\n        new_image = np.copy(image)\n    #Step 1: Create a clean list of lines\n    cleaned = []\n    for line in lines:\n        for x1,y1,x2,y2 in line:\n            if abs(y2-y1) <=1 and abs(x2-x1) >=25 and abs(x2-x1) <= 55:\n                cleaned.append((x1,y1,x2,y2))\n    \n    #Step 2: Sort cleaned by x1 position\n    import operator\n    list1 = sorted(cleaned, key=operator.itemgetter(0, 1))\n    \n    #Step 3: Find clusters of x1 close together - clust_dist apart\n    clusters = {}\n    dIndex = 0\n    clus_dist = 10\n\n    for i in range(len(list1) - 1):\n        distance = abs(list1[i+1][0] - list1[i][0])\n    #         print(distance)\n        if distance <= clus_dist:\n            if not dIndex in clusters.keys(): clusters[dIndex] = []\n            clusters[dIndex].append(list1[i])\n            clusters[dIndex].append(list1[i + 1])\n\n        else:\n            dIndex += 1\n    \n    #Step 4: Identify coordinates of rectangle around this cluster\n    rects = {}\n    i = 0\n    for key in clusters:\n        all_list = clusters[key]\n        cleaned = list(set(all_list))\n        if len(cleaned) > 5:\n            cleaned = sorted(cleaned, key=lambda tup: tup[1])\n            avg_y1 = cleaned[0][1]\n            avg_y2 = cleaned[-1][1]\n    #         print(avg_y1, avg_y2)\n            avg_x1 = 0\n            avg_x2 = 0\n            for tup in cleaned:\n                avg_x1 += tup[0]\n                avg_x2 += tup[2]\n            avg_x1 = avg_x1/len(cleaned)\n            avg_x2 = avg_x2/len(cleaned)\n            rects[i] = (avg_x1, avg_y1, avg_x2, avg_y2)\n            i += 1\n    \n    print(\"Num Parking Lanes: \", len(rects))\n    #Step 5: Draw the rectangles on the image\n    buff = 7\n    for key in rects:\n        tup_topLeft = (int(rects[key][0] - buff), int(rects[key][1]))\n        tup_botRight = (int(rects[key][2] + buff), int(rects[key][3]))\n#print(tup_topLeft, tup_botRight)\n        cv2.rectangle(new_image, tup_topLeft,tup_botRight,(0,255,0),3)\n    return new_image, rects\n\n# images showing the region of interest only\nrect_images = []\nrect_coords = []\nfor image, lines in zip(test_images, list_of_lines):\n    new_image, rects = identify_blocks(image, lines)\n    rect_images.append(new_image)\n    rect_coords.append(rects)\n    \nshow_images(rect_images)\n('Num Parking Lanes: ', 12)\n('Num Parking Lanes: ', 12)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Identify each spot and count num of parking spaces\nNext step-\n\n1.Based on width of each parking line segment into individual spots\n2.Draw a visualization of all parking spaces",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def draw_parking(image, rects, make_copy = True, color=[255, 0, 0], thickness=2, save = True):\n    if make_copy:\n        new_image = np.copy(image)\n    gap = 15.5\n    spot_dict = {} # maps each parking ID to its coords\n    tot_spots = 0\n    adj_y1 = {0: 20, 1:-10, 2:0, 3:-11, 4:28, 5:5, 6:-15, 7:-15, 8:-10, 9:-30, 10:9, 11:-32}\n    adj_y2 = {0: 30, 1: 50, 2:15, 3:10, 4:-15, 5:15, 6:15, 7:-20, 8:15, 9:15, 10:0, 11:30}\n    \n    adj_x1 = {0: -8, 1:-15, 2:-15, 3:-15, 4:-15, 5:-15, 6:-15, 7:-15, 8:-10, 9:-10, 10:-10, 11:0}\n    adj_x2 = {0: 0, 1: 15, 2:15, 3:15, 4:15, 5:15, 6:15, 7:15, 8:10, 9:10, 10:10, 11:0}\n    for key in rects:\n        # Horizontal lines\n        tup = rects[key]\n        x1 = int(tup[0]+ adj_x1[key])\n        x2 = int(tup[2]+ adj_x2[key])\n        y1 = int(tup[1] + adj_y1[key])\n        y2 = int(tup[3] + adj_y2[key])\n        cv2.rectangle(new_image, (x1, y1),(x2,y2),(0,255,0),2)\n        num_splits = int(abs(y2-y1)//gap)\n        for i in range(0, num_splits+1):\n            y = int(y1 + i*gap)\n            cv2.line(new_image, (x1, y), (x2, y), color, thickness)\n        if key > 0 and key < len(rects) -1 :        \n            #draw vertical lines\n            x = int((x1 + x2)/2)\n            cv2.line(new_image, (x, y1), (x, y2), color, thickness)\n        # Add up spots in this lane\n        if key == 0 or key == (len(rects) -1):\n            tot_spots += num_splits +1\n        else:\n            tot_spots += 2*(num_splits +1)\n           \n        # Dictionary of spot positions\n        if key == 0 or key == (len(rects) -1):\n            for i in range(0, num_splits+1):\n                cur_len = len(spot_dict)\n                y = int(y1 + i*gap)\n                spot_dict[(x1, y, x2, y+gap)] = cur_len +1        \n        else:\n            for i in range(0, num_splits+1):\n                cur_len = len(spot_dict)\n                y = int(y1 + i*gap)\n                x = int((x1 + x2)/2)\n                spot_dict[(x1, y, x, y+gap)] = cur_len +1\n                spot_dict[(x, y, x2, y+gap)] = cur_len +2   \n    \n    print(\"total parking spaces: \", tot_spots, cur_len)\n    if save:\n        filename = 'with_parking.jpg'\n        cv2.imwrite(filename, new_image)\n    return new_image, spot_dict\n\ndelineated = []\nspot_pos = []\nfor image, rects in zip(test_images, rect_coords):\n    new_image, spot_dict = draw_parking(image, rects)\n    delineated.append(new_image)\n    spot_pos.append(spot_dict)\n    show_images(delineated)     ",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "final_spot_dict = spot_pos[1]",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "print(len(final_spot_dict))",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def assign_spots_map(image, spot_dict=final_spot_dict, make_copy = True, color=[255, 0, 0], thickness=2):\n    if make_copy:\n        new_image = np.copy(image)\n    for spot in spot_dict.keys():\n        (x1, y1, x2, y2) = spot\n        cv2.rectangle(new_image, (int(x1),int(y1)), (int(x2),int(y2)), color, thickness)\n    return new_image\n\nmarked_spot_images = list(map(assign_spots_map, test_images))\nshow_images(marked_spot_images)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "### Save spot dictionary as pickle file\nimport pickle\n\nwith open('spot_dict.pickle', 'wb') as handle:\n    pickle.dump(final_spot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Save image for CNN model",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def save_images_for_cnn(image, spot_dict = final_spot_dict, folder_name ='for_cnn'):\n    for spot in spot_dict.keys():\n        (x1, y1, x2, y2) = spot\n        (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2))\n        #crop this image\n#         print(image.shape)\n        spot_img = image[y1:y2, x1:x2]\n        spot_img = cv2.resize(spot_img, (0,0), fx=2.0, fy=2.0) \n        spot_id = spot_dict[spot]\n        \n        filename = 'spot' + str(spot_id) +'.jpg'\n        print(spot_img.shape, filename, (x1,x2,y1,y2))\n        \n        cv2.imwrite(os.path.join(folder_name, filename), spot_img)\n        \n# save_images_for_cnn(test_images[0])",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Use trained CNN model to make predictions",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "## Imports for making predictions\nfrom PIL import Image\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.models import load_model\nfrom keras.preprocessing import image",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "cwd = os.getcwd()\ntop_model_weights_path = 'car1.h5'\n\nclass_dictionary = {}\nclass_dictionary[0] = 'empty'\nclass_dictionary[1] = 'occupied'",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from PIL import Image\nmodel = load_model(top_model_weights_path)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def make_prediction(image):\n    #Rescale image\n    img = image/255.\n\n    #Convert to a 4D tensor\n    image = np.expand_dims(img, axis=0)\n    #print(image.shape)\n\n    # make predictions on the preloaded model\n    class_predicted = model.predict(image)\n    inID = np.argmax(class_predicted[0])\n    label = class_dictionary[inID]\n    return label",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def predict_on_image(image, spot_dict = final_spot_dict, make_copy=True, color = [0, 255, 0], alpha=0.5):\n    if make_copy:\n        new_image = np.copy(image)\n        overlay = np.copy(image)\n    cnt_empty = 0\n    all_spots = 0\n    for spot in spot_dict.keys():\n        all_spots += 1\n        (x1, y1, x2, y2) = spot\n        (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2))\n        #crop this image\n        spot_img = image[y1:y2, x1:x2]\n        spot_img = cv2.resize(spot_img, (48, 48)) \n        \n        label = make_prediction(spot_img)\n#print(label)\n        if label == 'empty':\n            cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1)\n            cnt_empty += 1\n            \n    cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image)\n            \n    cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95),\n    cv2.FONT_HERSHEY_SIMPLEX,\n    0.7, (255, 255, 255), 2)\n    \n    cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125),\n    cv2.FONT_HERSHEY_SIMPLEX,\n    0.7, (255, 255, 255), 2)\n    save = False\n    \n     if save:\n        filename = 'with_marking.jpg'\n        cv2.imwrite(filename, new_image)\n    \n    return new_image\n\n\npredicted_images = list(map(predict_on_image, test_images))\nshow_images(predicted_images)",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "#Run code on video",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "video_name = 'cars8_stat3.mp4'\ncap = cv2.VideoCapture(video_name)\nret = True\ncount = 0\n\nwhile ret:\n        ret, image = cap.read()\n        count += 1\n        if count == 5:\n            count = 0\n            \n            new_image = np.copy(image)\n            overlay = np.copy(image)\n            cnt_empty = 0\n            all_spots = 0\n            color = [0, 255, 0] \n            alpha=0.5\n            for spot in final_spot_dict.keys():\n                all_spots += 1\n                (x1, y1, x2, y2) = spot\n                (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2))\n                #crop this image\n                spot_img = image[y1:y2, x1:x2]\n                spot_img = cv2.resize(spot_img, (48, 48)) \n\n                label = make_prediction(spot_img)\n#print(label)\n                if label == 'empty':\n                    cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1)\n                    cnt_empty += 1\n                cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image)\n\n            cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.7, (255, 255, 255), 2)\n\n            cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125),\n            cv2.FONT_HERSHEY_SIMPLEX,\n            0.7, (255, 255, 255), 2)\n            cv2.imshow('frame', new_image)\n            if cv2.waitKey(10) & 0xFF == ord('q'):\n                break\n        #out.write(image)\n\ncv2.destroyAllWindows()\ncap.release()",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}